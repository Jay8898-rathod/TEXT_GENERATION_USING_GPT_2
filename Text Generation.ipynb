{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOmWJYTB2IIJrFMbiEe/49P"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# TEXT GENERATION USING GPT 2 MODEL"],"metadata":{"id":"yU_JpyGQZL3Y"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"DWKk4BiaY21e"},"outputs":[],"source":["import gradio as gr\n","import tensorflow as tf\n","from transformers import TFGPT2LMHeadModel, GPT2Tokenizer"]},{"cell_type":"markdown","source":["# **Loading the model and creating the generate function**"],"metadata":{"id":"0eWaiyS2Z53-"}},{"cell_type":"code","source":["tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n","model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id = tokenizer.eos_token_id)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-5HS7U8zZnqF","executionInfo":{"status":"ok","timestamp":1692611395489,"user_tz":-330,"elapsed":3776,"user":{"displayName":"jay rathod","userId":"03801049484985404200"}},"outputId":"d68b9f68-395b-462a-dcb3-04f21be29290"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n","\n","All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"]}]},{"cell_type":"code","source":["def generate_text(inp):\n","    input_ids = tokenizer.encode(inp, return_tensors='tf')\n","    beam_output = model.generate(input_ids, max_length = 100, num_beams = 5, no_repeat_ngram_size = 2, early_stopping = True)\n","    output = tokenizer.decode(beam_output[0], skip_special_tokens = True, clean_up_tokenization_spaces = True)\n","    return \".\".join(output.split(\".\")[:-1])+\".\""],"metadata":{"id":"i6amOF-2aWJk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["output_text = gr.outputs.Textbox()\n","gr.Interface(generate_text, \"textbox\", output_text, title = \"GPT-2\", description = \"TEXT GENERATION USING GPT 2 MODEL\").launch(share=True, debug=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":646},"id":"-aHAeT14a_89","outputId":"488d1673-27f9-46a3-9c28-75f36f2f0bed"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["<ipython-input-12-bd16e0dcb2b7>:1: GradioDeprecationWarning: Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\n","  output_text = gr.outputs.Textbox()\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n","Running on public URL: https://e9e2448523ee6f0cf3.gradio.live\n","\n","This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"]},{"data":{"text/html":["<div><iframe src=\"https://e9e2448523ee6f0cf3.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}]},{"cell_type":"code","source":[],"metadata":{"id":"NJwNPNSlbYci"},"execution_count":null,"outputs":[]}]}